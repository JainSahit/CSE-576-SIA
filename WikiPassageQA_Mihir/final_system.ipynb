{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_system.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3540ba836844ed2ab9c30be07b97358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ed675025bb334a8eb51da19deba9c52b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2c1e9d0e494e45e18ccac0750ef8724d",
              "IPY_MODEL_681637680d52414cabe9d508fc108f39"
            ]
          }
        },
        "ed675025bb334a8eb51da19deba9c52b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c1e9d0e494e45e18ccac0750ef8724d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_42886e2a804848c2899bcde2f1143e85",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29e8b08d62c740f4a4ad7f854e8e36f0"
          }
        },
        "681637680d52414cabe9d508fc108f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b3247b1f7fb948eda821f078919d3cd8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 1.29MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06a43d5733304b0dacbde09552837977"
          }
        },
        "42886e2a804848c2899bcde2f1143e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29e8b08d62c740f4a4ad7f854e8e36f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3247b1f7fb948eda821f078919d3cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06a43d5733304b0dacbde09552837977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1275e9fb5c594ab7aca996e98dc72094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_de5c84cc05b54e768fff246259e0eed4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6fbf8c51ef674f919d7305f16a895545",
              "IPY_MODEL_372de373bd174cd3b4fde1d616658bd0"
            ]
          }
        },
        "de5c84cc05b54e768fff246259e0eed4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6fbf8c51ef674f919d7305f16a895545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d9ca63ca01244e03a32e089adef44bc5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7de8bbf43894433ea35e513bb86100bc"
          }
        },
        "372de373bd174cd3b4fde1d616658bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ae97a64faa034ac1bbd65e6c722ade7d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 2.24MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8d724fa2a004652a89e9d0e657736fc"
          }
        },
        "d9ca63ca01244e03a32e089adef44bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7de8bbf43894433ea35e513bb86100bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae97a64faa034ac1bbd65e6c722ade7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8d724fa2a004652a89e9d0e657736fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mepnd6FRrIp4",
        "outputId": "1d851206-04f0-48b3-c794-622549b2d918"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUKKqzNotSj-",
        "outputId": "79ed4a35-223e-4e16-8275-5398abcc8504"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=7228e7063f348458edae0d20a56e33d251fa884aed939fc8e5b8e40fec891b9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 26.4 GB  |     Proc size: 111.5 MB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total     16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ0n5N3LtZ0E",
        "outputId": "82987a22-886d-4046-cb90-bdc2d00cab00"
      },
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install unidecode\n",
        "!pip3 install pyserini"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 37.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=9d42ff2ad554d6a12f40ae401003a62c7be58ac7de599733969e83c1effc9dcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 6.8MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n",
            "Collecting pyserini\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/f9/3e0a58b1130863276855410e22f94351f45da5516cbabd65ef57fed4348c/pyserini-0.10.0.0-py3-none-any.whl (63.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63.1MB 100kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyserini) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyserini) (1.18.5)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from pyserini) (0.29.21)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyserini) (1.1.4)\n",
            "Collecting pyjnius\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/50/098cb5fb76fb7c7d99d403226a2a63dcbfb5c129b71b7d0f5200b05de1f0/pyjnius-1.3.0-cp36-cp36m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 37.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyserini) (0.22.2.post1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pyserini) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pyserini) (2.8.1)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from pyjnius->pyserini) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyserini) (0.17.0)\n",
            "Installing collected packages: pyjnius, pyserini\n",
            "Successfully installed pyjnius-1.3.0 pyserini-0.10.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lRm1-KIul8C"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "import unidecode\n",
        "import re\n",
        "import logging\n",
        "from tqdm.notebook import tnrange\n",
        "import glob\n",
        "import json\n",
        "\n",
        "#For ploting results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# DL Libraries\n",
        "from transformers import BertModel, AdamW, BertTokenizer, BertConfig, RobertaTokenizer, RobertaModel, RobertaForSequenceClassification\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statistics import mean \n",
        "\n",
        "#For anserini\n",
        "from pyserini.search import SimpleSearcher\n",
        "from pyserini import analysis, index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VE-MTHzu7z6"
      },
      "source": [
        "indexes= '/content/drive/My Drive/project_nlp/submission/lucene_indexing'\n",
        "\n",
        "searcher = SimpleSearcher(join(indexes, 'pyserini/indexes/lucene-index-wiki'))\n",
        "searcher.set_bm25(0.4, 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX57UMlFffTZ",
        "outputId": "75f28a2c-cb43-41ce-ad68-7688ab395ec9"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"device: {} n_gpu: {}\".format(device, n_gpu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda n_gpu: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTOxR5W-fxkN"
      },
      "source": [
        "##Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhppChMHf0B3"
      },
      "source": [
        "def create_dataloader(tokenizer, df):\n",
        "    input_ids= list()\n",
        "    attention_masks= list()\n",
        "\n",
        "    special_sentences_1 = [sentence for i, sentence in enumerate(df.question)]\n",
        "    special_sentences_2 = [\" [SEP] \" + str(sentence) for i, sentence in enumerate(df.answer)]\n",
        "    special_sentences = [i + j for i, j in zip(special_sentences_1, special_sentences_2)]\n",
        "\n",
        "    for sentence in special_sentences:\n",
        "      encoded_text = tokenizer.encode_plus(sentence, max_length=512, add_special_tokens=True, return_token_type_ids=False, \n",
        "                                       padding='max_length', return_attention_mask=True, truncation=True)\n",
        "      input_ids.append(encoded_text['input_ids'])\n",
        "      attention_masks.append(encoded_text['attention_mask'])\n",
        "\n",
        "    inputs = torch.tensor(input_ids).to(device)\n",
        "    masks = torch.tensor(attention_masks).to(device)\n",
        "    # gold_labels = torch.tensor(df.label.tolist()).to(device)\n",
        "  \n",
        "    data = TensorDataset(inputs, masks)\n",
        "    sampler = SequentialSampler(data)\n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=1)\n",
        "\n",
        "    return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oISTRw0NySy2"
      },
      "source": [
        "##evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXTNsCUQyUda"
      },
      "source": [
        "def precision_at_k(r, k):\n",
        "  assert k >= 1\n",
        "  r = np.asarray(r)[:k] != 0\n",
        "  if r.size != k:\n",
        "    raise ValueError('Relevance score length < k')\n",
        "  return np.mean(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXXRoQg5k_Im"
      },
      "source": [
        "def recall_at_k(actual, predicted, k):\n",
        "  count=0\n",
        "  predicted= predicted[0:k]\n",
        "  for a in actual:\n",
        "    if a in predicted:\n",
        "      count+=1\n",
        "  return count/len(actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0qz9ZHrybRK"
      },
      "source": [
        "def average_precision(r):\n",
        "  r = np.asarray(r) != 0\n",
        "  out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
        "  if not out:\n",
        "    return 0\n",
        "  \n",
        "  return np.mean(out)\n",
        "\n",
        "def mean_average_precision(rs):\n",
        "  return np.mean([average_precision(r) for r in rs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c5FNqwmyovm"
      },
      "source": [
        "def mean_reciprocal_rank(rs):\n",
        "  rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
        "  return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_Z9JoXZjFVi"
      },
      "source": [
        "##Model and Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu59qsVp2JhY"
      },
      "source": [
        "def get_candidate_passages(query):\n",
        "\n",
        "  candidate_passages=[]\n",
        "  hits = searcher.search(query, k=20)\n",
        "  # return the first top 10 hits:\n",
        "  for hit in hits:\n",
        "    doc = searcher.doc(str(hit.docid))\n",
        "    candidate_passages.append(doc.raw().replace('\"', ''))\n",
        "  \n",
        "  return candidate_passages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LodYuRB_exjc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "e3540ba836844ed2ab9c30be07b97358",
            "ed675025bb334a8eb51da19deba9c52b",
            "2c1e9d0e494e45e18ccac0750ef8724d",
            "681637680d52414cabe9d508fc108f39",
            "42886e2a804848c2899bcde2f1143e85",
            "29e8b08d62c740f4a4ad7f854e8e36f0",
            "b3247b1f7fb948eda821f078919d3cd8",
            "06a43d5733304b0dacbde09552837977",
            "1275e9fb5c594ab7aca996e98dc72094",
            "de5c84cc05b54e768fff246259e0eed4",
            "6fbf8c51ef674f919d7305f16a895545",
            "372de373bd174cd3b4fde1d616658bd0",
            "d9ca63ca01244e03a32e089adef44bc5",
            "7de8bbf43894433ea35e513bb86100bc",
            "ae97a64faa034ac1bbd65e6c722ade7d",
            "f8d724fa2a004652a89e9d0e657736fc"
          ]
        },
        "outputId": "60f49f45-f502-408e-82bf-3e528e112f03"
      },
      "source": [
        "#classification model\n",
        "\n",
        "model_path= '/content/drive/MyDrive/man_mihir_project/cls_experiment/model_sqcls'\n",
        "\n",
        "model= RobertaForSequenceClassification.from_pretrained(model_path)\n",
        "model.to(device)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3540ba836844ed2ab9c30be07b97358",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1275e9fb5c594ab7aca996e98dc72094",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74M2hPi5vIg4"
      },
      "source": [
        "load_data = '/content/drive/My Drive/man_mihir_project/data'\n",
        "\n",
        "with open(join(load_data,'document_passages.json'),'r') as f:\n",
        "  doc_passages= json.load(f)\n",
        "\n",
        "data_df= pd.read_csv(join(load_data, 'test.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW87wjQ4wRVH",
        "outputId": "a2cd9ecb-e0b0-4f32-fd9a-0e22aa47e067"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "\n",
        "for index, row in data_df.iterrows():\n",
        "  answers= get_candidate_passages(row.Question)\n",
        "  temp_df= pd.DataFrame(answers, columns=['answer'])\n",
        "  temp_df['question']=row.Question\n",
        "  dataloader= create_dataloader(tokenizer, temp_df)\n",
        "  pos_prob= []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      ip_ids, masks= batch\n",
        "      logits = model(ip_ids, attention_mask=masks)\n",
        "      logits= logits[0].squeeze(0)\n",
        "      pred_logits = logits.cpu().detach().numpy()\n",
        "      pos_prob.append(pred_logits[1])\n",
        "  \n",
        "  temp_df['score']=pos_prob\n",
        "  temp_df= temp_df.sort_values(by='score', ascending=False)\n",
        "  top_k_ans= temp_df[0:20].answer.tolist()\n",
        "\n",
        "  doc= doc_passages[str(row.DocumentID)]\n",
        "  passages_no= row.RelevantPassages.split(',')\n",
        "  org_ans=[]\n",
        "  for no in passages_no:\n",
        "    org_ans.append(doc[str(no)])\n",
        "\n",
        "  r=[]\n",
        "  for ans in top_k_ans:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "  \n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, top_k_ans, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, top_k_ans, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, top_k_ans, 20))\n",
        "\n",
        "print(mean(precision_at_5))\n",
        "print(mean(precision_at_10))\n",
        "print(mean(recall_at_5))\n",
        "print(mean(recall_at_10))\n",
        "print(mean(recall_at_20))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09134615384615385\n",
            "0.05480769230769231\n",
            "0.22129693223443223\n",
            "0.2473385989010989\n",
            "0.2725789835164835\n",
            "0.3025580595168406\n",
            "0.31782559250390136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UpMx2QBh9CH"
      },
      "source": [
        "##SIA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvwa9PPBh--s"
      },
      "source": [
        "#Class for Regression\n",
        "class Regressor(nn.Module):\n",
        "\n",
        "  def __init__(self, model_path):\n",
        "    super(Regressor, self).__init__()\n",
        "    self.bert = RobertaModel.from_pretrained(model_path)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    output, pooler_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    score= self.out(pooler_out)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsw_WqQZiIsA"
      },
      "source": [
        "#Model Intialization\n",
        "\n",
        "model_path= '/content/drive/MyDrive/man_mihir_project/sia_experiment/model'\n",
        "\n",
        "#Load Model\n",
        "model= Regressor(model_path)\n",
        "lr_weights= torch.load(join(model_path, 'model_state.bin'))\n",
        "model.out.load_state_dict(lr_weights)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao96IjuajOoW",
        "outputId": "fb7662d5-c070-46d2-e2c5-f88773fc6357"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "\n",
        "for index, row in data_df.iterrows():\n",
        "  answers= get_candidate_passages(row.Question)\n",
        "  temp_df= pd.DataFrame(answers, columns=['answer'])\n",
        "  temp_df['question']=row.Question\n",
        "  dataloader= create_dataloader(tokenizer, temp_df)\n",
        "  pos_prob= []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      ip_ids, masks= batch\n",
        "      score = model(ip_ids, attention_mask=masks)\n",
        "      pos_prob.append(score.cpu().detach().numpy())\n",
        "  \n",
        "  temp_df['score']=pos_prob\n",
        "  temp_df= temp_df.sort_values(by='score', ascending=False)\n",
        "  top_k_ans= temp_df[0:20].answer.tolist()\n",
        "\n",
        "  doc= doc_passages[str(row.DocumentID)]\n",
        "  passages_no= row.RelevantPassages.split(',')\n",
        "  org_ans=[]\n",
        "  for no in passages_no:\n",
        "    org_ans.append(doc[str(no)])\n",
        "\n",
        "  r=[]\n",
        "  for ans in top_k_ans:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "  \n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, top_k_ans, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, top_k_ans, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, top_k_ans, 20))\n",
        "\n",
        "print(mean(precision_at_5))\n",
        "print(mean(precision_at_10))\n",
        "print(mean(recall_at_5))\n",
        "print(mean(recall_at_10))\n",
        "print(mean(recall_at_20))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09230769230769231\n",
            "0.054086538461538464\n",
            "0.22410141941391942\n",
            "0.24773923992673993\n",
            "0.264566163003663\n",
            "0.2992625758702875\n",
            "0.3089393619862184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3okr25qjYxho"
      },
      "source": [
        "##Anserini Only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QeJGPv4Yznl",
        "outputId": "6ac3ad89-5bc5-4f8e-e9a1-fd9ad14fcfff"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "count=0\n",
        "\n",
        "for index, row in data_df.iterrows():\n",
        "  answers= get_candidate_passages(row.Question)\n",
        "  doc= doc_passages[str(row.DocumentID)]\n",
        "  passages_no= row.RelevantPassages.split(',')\n",
        "  org_ans=[]\n",
        "  for no in passages_no:\n",
        "    org_ans.append(doc[str(no)])\n",
        "\n",
        "  r=[]\n",
        "  for ans in answers:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "\n",
        "  if max(r)==0:\n",
        "    count+=1\n",
        "  \n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, answers, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, answers, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, answers, 20))\n",
        "\n",
        "print(mean(precision_at_5))\n",
        "print(mean(precision_at_10))\n",
        "print(mean(recall_at_5))\n",
        "print(mean(recall_at_10))\n",
        "print(mean(recall_at_20))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06490384615384616\n",
            "0.04014423076923077\n",
            "0.19811698717948717\n",
            "0.24573603479853479\n",
            "0.2825950091575092\n",
            "0.21090602390195673\n",
            "0.21888767924682356\n",
            "247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEe-5c02XK49"
      },
      "source": [
        "##BM25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9FquSx7vXkS",
        "outputId": "0c6a7cf5-3ae2-41f1-d5b7-f0325d7cec20"
      },
      "source": [
        "!pip3 install rank_bm25"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.18.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM5utknpX1KZ"
      },
      "source": [
        "passages= list()\n",
        "for key in doc_passages:\n",
        "  passages.extend(list(doc_passages[str(key)].values()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJzioOhyvSBr"
      },
      "source": [
        "from rank_bm25 import BM25Okapi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a06Gipg-vdTu"
      },
      "source": [
        "tokenized_corpus = [doc.split(\" \") for doc in passages]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1bgbtr9vl79"
      },
      "source": [
        "bm25 = BM25Okapi(tokenized_corpus, k1=0.4, b=0.1, epsilon=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEbX3m9Mv3A6",
        "outputId": "31113a4e-f7c1-4ef5-a85a-d706c4b94e7f"
      },
      "source": [
        "precision_at_5=[]\n",
        "rs=[]\n",
        "count=0\n",
        "\n",
        "for index, row in data_df.iterrows():\n",
        "\n",
        "  query = row.Question\n",
        "  tokenized_query = query.split(\" \")\n",
        "\n",
        "  answers= bm25.get_top_n(tokenized_query, passages, n=5)\n",
        "\n",
        "  doc= doc_passages[str(row.DocumentID)]\n",
        "  passages_no= row.RelevantPassages.split(',')\n",
        "  org_ans=[]\n",
        "  for no in passages_no:\n",
        "    org_ans.append(doc[str(no)])\n",
        "\n",
        "  r=[]\n",
        "  for ans in answers:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "\n",
        "  if max(r)==0:\n",
        "    count+=1\n",
        "  \n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "\n",
        "print(mean(precision_at_5))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07067307692307692\n",
            "0.2255742521367521\n",
            "0.22932692307692307\n",
            "286\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
