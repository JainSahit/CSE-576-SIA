{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1218506822_EvaluationMetrics_BM25_SIA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5850c793b2f94dfda553abf9fa2ea9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_99ac8abc0d814455a16ccf41eb6b2c63",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2333ca99e2d14d2c963ec2b79ecd4681",
              "IPY_MODEL_1c7e8b4d0cef42eb9539cb66edcb976d"
            ]
          }
        },
        "99ac8abc0d814455a16ccf41eb6b2c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2333ca99e2d14d2c963ec2b79ecd4681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e96dbf7fe47d458e8acfdf91cb1cd3f7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 7405,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 7405,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af71269ffd1c448092f38c049e3c85fa"
          }
        },
        "1c7e8b4d0cef42eb9539cb66edcb976d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8eac2e888160494e851a4ca2e6575902",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 7405/7405 [00:21&lt;00:00, 341.94it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7fe85656f9f04f01bd502ac2f353083e"
          }
        },
        "e96dbf7fe47d458e8acfdf91cb1cd3f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af71269ffd1c448092f38c049e3c85fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8eac2e888160494e851a4ca2e6575902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7fe85656f9f04f01bd502ac2f353083e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59beab1e60574211b78658e61f912dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5862c39e769449229c2730252155646c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c2d4a6ffbf144a19848fa6280492c603",
              "IPY_MODEL_3e342b9bf194419384556fb01706b46d"
            ]
          }
        },
        "5862c39e769449229c2730252155646c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c2d4a6ffbf144a19848fa6280492c603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a831ce2136f74039828ad7b0a49d77eb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_43e4b5c9efcd4a0199722908c2d31f26"
          }
        },
        "3e342b9bf194419384556fb01706b46d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ed78796a66f745ab9b3f703c5f1ff2b2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 4.37MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4ff6f4bdb72483abb430497242c7526"
          }
        },
        "a831ce2136f74039828ad7b0a49d77eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "43e4b5c9efcd4a0199722908c2d31f26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed78796a66f745ab9b3f703c5f1ff2b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4ff6f4bdb72483abb430497242c7526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "230297ed4ac342eba963c9f517c8bc87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_503a46a458644170bbaf7b6f48103d74",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b9c18a54531d4570b0455fdab8fb6ec1",
              "IPY_MODEL_f7c42494efb34251a830acfe1d044cce"
            ]
          }
        },
        "503a46a458644170bbaf7b6f48103d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9c18a54531d4570b0455fdab8fb6ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_075e7ca44a7c4a968975ffaf4adbf845",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67a885628b594e14b3dc2836a36b5769"
          }
        },
        "f7c42494efb34251a830acfe1d044cce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5de3df6f542c4842988ce173586608cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 3.11MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0aed7a693fbc45b2acca38d3db91da19"
          }
        },
        "075e7ca44a7c4a968975ffaf4adbf845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67a885628b594e14b3dc2836a36b5769": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5de3df6f542c4842988ce173586608cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0aed7a693fbc45b2acca38d3db91da19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn4XsWbJY51T",
        "outputId": "fbe6be91-76bb-4054-f253-f24daadbaa44"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3e1rjCSgcOo"
      },
      "source": [
        "!pip install pyserini==0.9.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfvbEAeWZaan"
      },
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccy-vJ75Z3mj",
        "outputId": "adc3e042-9766-4db0-f164-a8291e2259fa"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "import unidecode\n",
        "import re\n",
        "import logging\n",
        "from tqdm.notebook import tnrange\n",
        "import glob\n",
        "import json\n",
        "\n",
        "#For ploting results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# DL Libraries\n",
        "from transformers import BertModel, AdamW, BertTokenizer,RobertaTokenizer, BertConfig, get_linear_schedule_with_warmup,RobertaModel, RobertaForSequenceClassification\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,TensorDataset)\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statistics import mean \n",
        "#NLTK Libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgzfI8x8yDuX",
        "outputId": "f29d17b9-8046-4e49-dfe3-ecf9331b7238"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=6b6fefa8420998974efd213656d52f47304c5b5fde51e67f9ed14a6b654f7feb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.5 GB  |     Proc size: 544.2 MB\n",
            "GPU RAM Free: 15069MB | Used: 10MB | Util   0% | Total     15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4X4AJdmqS2_",
        "outputId": "9f164e08-4eb0-4f3e-fd78-dee50cbf60b1"
      },
      "source": [
        "from pyserini.search import SimpleSearcher\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "\n",
        "# searcher = SimpleSearcher('/content/drive/My Drive/QASC-DATASET/data/QASC_Corpus/anserini/pyserini/indexes/lucene-index-qasc')\n",
        "searcher = SimpleSearcher('/content/drive/MyDrive/Courses/NLP/Project/data/HOTPOTQA_Corpus/anserini/pyserini/indexes/lucene-index-hotpotqa')\n",
        "\n",
        "# /content/drive/MyDrive/Courses/NLP/Project/data/HOTPOTQA_Corpus/anserini/pyserini/indexes/lucene-index-hotpotqa\n",
        "\n",
        "searcher.set_bm25(1.2, 0.75)\n",
        "\n",
        "hits = searcher.search(\"The American\")\n",
        "\n",
        "# Print the first 10 hits:\n",
        "for i in range (0,len(hits)):\n",
        "    print(f'{hits[i].docid:15} {hits[i].score:.5f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12210           1.25990\n",
            "104021          1.21460\n",
            "77994           1.20960\n",
            "89711           1.20960\n",
            "95879           1.20960\n",
            "2037            1.20210\n",
            "47974           1.18920\n",
            "29845           1.18660\n",
            "41591           1.18660\n",
            "95881           1.18660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkD7sVgOhQH7",
        "outputId": "94108a30-e992-4a64-9087-51ec634ad39e"
      },
      "source": [
        "for i in range (0,len(hits)):\n",
        "  doc = searcher.doc(str(hits[i].docid))\n",
        "  print(doc.raw())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Italian Americans (Italian: \"\"italoamericani \"\"or\"\" italo-americani\"\" ] ) are an ethnic group consisting of Americans who have full or partial ancestry from Italy. Italian Americans are the fourth largest European ethnic group in the United States (not including American ethnicity, an ethnonym used by many in the United States; overall, Italian Americans rank seventh, behind German American, Irish American, African American, English American, American, and Mexican American)..\"\n",
            "Indian Americans or Indo-Americans are Americans whose ancestry belongs to any of the many ethnic groups of the Republic of India. As the most socio-economically successful minority ethnic group in the U.S., Indian Americans comprise 4 million people, representing around 1% of the U.S. population as of 2015. Indian Americans are the country's third-largest Asian group alone or in combination with other races after Chinese Americans and Filipino Americans, according to 2015 American Community Survey data. The U.S. Census Bureau uses the term Asian Indian to avoid confusion with the indigenous peoples of the Americas commonly referred to as American Indians (or Native Americans).\n",
            "\"American Wedding (known as American Pie 3: The Wedding or American Pie: The Wedding, in some countries) is a 2003 American sex comedy film and a sequel to \"\"American Pie\"\" and \"\"American Pie 2\"\". It is the third (originally intended final) installment in the \"\"American Pie\"\" theatrical series. It was written by Adam Herz and directed by Jesse Dylan. Another sequel, \"\"American Reunion\"\", was released nine years later. This also stands as the last film in the series to be written by Herz, who conceptualized the franchise.\"\n",
            "\"American Wedding (known as American Pie 3: The Wedding or American Pie: The Wedding, in some countries) is a 2003 American sex comedy film and a sequel to \"\"American Pie\"\" and \"\"American Pie 2\"\". It is the third (originally intended final) installment in the \"\"American Pie\"\" theatrical series. It was written by Adam Herz and directed by Jesse Dylan. Another sequel, \"\"American Reunion\"\", was released nine years later. This also stands as the last film in the series to be written by Herz, who conceptualized the franchise.\"\n",
            "\"American Wedding (known as American Pie 3: The Wedding or American Pie: The Wedding, in some countries) is a 2003 American sex comedy film and a sequel to \"\"American Pie\"\" and \"\"American Pie 2\"\". It is the third (originally intended final) installment in the \"\"American Pie\"\" theatrical series. It was written by Adam Herz and directed by Jesse Dylan. Another sequel, \"\"American Reunion\"\", was released nine years later. This also stands as the last film in the series to be written by Herz, who conceptualized the franchise.\"\n",
            "\"The American Beauty/American Psycho Tour was a concert tour by American rock band Fall Out Boy. Supporting the band's sixth studio album \"\"American Beauty/American Psycho\"\" (2015), the tour visited North America and Europe in 2015. The North American leg was co-headlined with American rapper Wiz Khalifa under the name The Boys of Zummer. The Boys of Zummer leg with Wiz Khalifa ranked fifty-ninth for Pollstar's Year End Top 200 North American Tours of 2015, grossing $18.2 million.\"\n",
            "Mike Barbarick is a retired American soccer goalkeeper who played professionally in the North American Soccer League, American Soccer League, United Soccer League and American Indoor Soccer Association.\n",
            "\"Chris Owen (born September 25, 1980) is an American actor and photographer. He is best known for his role as The Sherminator in the \"\"American Pie film franchise\"\", appearing in \"\"American Pie\"\", \"\"American Pie 2\"\", \"\"\"\" and \"\"American Reunion\"\". Aside from Eugene Levy, he is the only actor from the theatrical features to appear in the \"\"\"\"American Pie Presents:\"\"\"\" direct-to-video spin-off movies.\"\n",
            "\"Chris Owen (born September 25, 1980) is an American actor and photographer. He is best known for his role as The Sherminator in the \"\"American Pie film franchise\"\", appearing in \"\"American Pie\"\", \"\"American Pie 2\"\", \"\"\"\" and \"\"American Reunion\"\". Aside from Eugene Levy, he is the only actor from the theatrical features to appear in the \"\"\"\"American Pie Presents:\"\"\"\" direct-to-video spin-off movies.\"\n",
            "\"Chris Owen (born September 25, 1980) is an American actor and photographer. He is best known for his role as The Sherminator in the \"\"American Pie film franchise\"\", appearing in \"\"American Pie\"\", \"\"American Pie 2\"\", \"\"\"\" and \"\"American Reunion\"\". Aside from Eugene Levy, he is the only actor from the theatrical features to appear in the \"\"\"\"American Pie Presents:\"\"\"\" direct-to-video spin-off movies.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qrEBtXG1od0",
        "outputId": "6b34b38f-d67a-42af-86fc-13a75b9eb315"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"device: {} n_gpu: {}\".format(device, n_gpu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device: cuda n_gpu: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KquAiIkAzHEL"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQY9J8wvzJi1"
      },
      "source": [
        "def create_dataloader(tokenizer, df):\n",
        "    input_ids= list()\n",
        "    attention_masks= list()\n",
        "\n",
        "    special_sentences_1 = [sentence for i, sentence in enumerate(df.question)]\n",
        "    special_sentences_2 = [\" [SEP] \" + str(sentence) for i, sentence in enumerate(df.answer)]\n",
        "    special_sentences = [i + j for i, j in zip(special_sentences_1, special_sentences_2)]\n",
        "\n",
        "    for sentence in special_sentences:\n",
        "      encoded_text = tokenizer.encode_plus(sentence, max_length=512, add_special_tokens=True, return_token_type_ids=False, \n",
        "                                       padding='max_length', return_attention_mask=True, truncation=True)\n",
        "      input_ids.append(encoded_text['input_ids'])\n",
        "      attention_masks.append(encoded_text['attention_mask'])\n",
        "\n",
        "    inputs = torch.tensor(input_ids).to(device)\n",
        "    masks = torch.tensor(attention_masks).to(device)\n",
        "    # gold_labels = torch.tensor(df.label.tolist()).to(device)\n",
        "  \n",
        "    data = TensorDataset(inputs, masks)\n",
        "    sampler = SequentialSampler(data)\n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=1)\n",
        "\n",
        "    return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEpV0Kvi0GwN"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZwVfyc70GY2"
      },
      "source": [
        "def precision_at_k(r, k):\n",
        "    assert k >= 1\n",
        "    r = np.asarray(r)[:k] != 0\n",
        "    if r.size != k:\n",
        "        raise ValueError('Relevance score length < k')\n",
        "    return np.mean(r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke5bRQiO0Lkq"
      },
      "source": [
        "def recall_at_k(actual, predicted, k):\n",
        "  count=0\n",
        "  predicted= predicted[0:k]\n",
        "  for a in actual:\n",
        "    if a in predicted:\n",
        "      count+=1\n",
        "  result = count/len(actual)\n",
        "  # print(\"count=\",count,\"length\",len(actual),\"result\", result)\n",
        "  return count/len(actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apDEexVC0NlR"
      },
      "source": [
        "def average_precision(r):\n",
        "    r = np.asarray(r) != 0\n",
        "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
        "    if not out:\n",
        "        return 0.\n",
        "    return np.mean(out)\n",
        "\n",
        "def mean_average_precision(rs):\n",
        "    return np.mean([average_precision(r) for r in rs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTHoFvIV0Puc"
      },
      "source": [
        "def mean_reciprocal_rank(rs):\n",
        "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
        "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPw0_Vox0T7o"
      },
      "source": [
        "#Fetch sentences from corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGu4UgmT0U3Y"
      },
      "source": [
        "def get_candidate_passages(query):\n",
        "\n",
        "  candidate_passages=[]\n",
        "  hits = searcher.search(query, k=50)\n",
        "  # return the first top 10 hits:\n",
        "  for hit in hits:\n",
        "    doc = searcher.doc(str(hit.docid))\n",
        "    candidate_passages.append(doc.raw().replace('\"', ''))\n",
        "  \n",
        "  return candidate_passages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkuKgZOS0a7d"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvmpnz3v2bE3"
      },
      "source": [
        "#Class for Regression\n",
        "class Regressor(nn.Module):\n",
        "\n",
        "  def __init__(self, model_path):\n",
        "    super(Regressor, self).__init__()\n",
        "    self.bert = RobertaModel.from_pretrained(model_path)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    output, pooler_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    score= self.out(pooler_out)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csH_M4d00iQU"
      },
      "source": [
        "#classification model\n",
        "\n",
        "# model_path= '/content/drive/My Drive/QASC-DATASET/data/QASC_Dataset/sia_experiment/model_QASCrobertabase_40ktraindataset_10epochs'\n",
        "model_path= '/content/drive/MyDrive/Courses/NLP/Project/models/sia_trained_roberta_model'\n",
        "\n",
        "#Load Model\n",
        "model= Regressor(model_path)\n",
        "lr_weights= torch.load(join(model_path, 'model_state.bin'))\n",
        "model.out.load_state_dict(lr_weights)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6GAYHmKaDT7"
      },
      "source": [
        "# Load Dev set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sO_6mO6ejcKc"
      },
      "source": [
        "def generate_supfacts_list(df, start,end):\n",
        "    new_df = pd.DataFrame(columns=['question','answer'])   \n",
        "\n",
        "    for i in tnrange(start,end):\n",
        "      \n",
        "      cntxt_len = len(df['context'][i])\n",
        "      supporting_facts = df['supporting_facts'][i]\n",
        "      question =  df['question'][i]\n",
        "      fact_list = []\n",
        "      for fact in  supporting_facts:\n",
        "        fact_title = fact[0]\n",
        "        for j  in range(0, cntxt_len):\n",
        "          passage_title =  df['context'][i][j][0]\n",
        "          if fact_title == passage_title:\n",
        "            passage  = df['context'][i][j][1]\n",
        "            text  = \"\".join(passage)\n",
        "            fact_list.append(text)\n",
        "              \n",
        "      new_row = { 'question':question,'answer': fact_list }\n",
        "      new_df =  new_df.append(new_row,ignore_index= True)\n",
        "\n",
        "    return  new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5850c793b2f94dfda553abf9fa2ea9e4",
            "99ac8abc0d814455a16ccf41eb6b2c63",
            "2333ca99e2d14d2c963ec2b79ecd4681",
            "1c7e8b4d0cef42eb9539cb66edcb976d",
            "e96dbf7fe47d458e8acfdf91cb1cd3f7",
            "af71269ffd1c448092f38c049e3c85fa",
            "8eac2e888160494e851a4ca2e6575902",
            "7fe85656f9f04f01bd502ac2f353083e"
          ]
        },
        "id": "Dcp2jfnjjh8F",
        "outputId": "f664e4eb-7aee-4cdc-91a9-c67045a5dcef"
      },
      "source": [
        "\n",
        "start = 0\n",
        "end = 7405\n",
        "FOLDER = '/content/drive/My Drive/Courses/NLP/Project/data'\n",
        "JSON_FILE =  FOLDER +\"/hotpot_dev_distractor_v1.json\"\n",
        "df = pd.read_json(JSON_FILE)\n",
        "data_df = generate_supfacts_list(df, start,end)\n",
        "data_df.to_csv('/content/drive/MyDrive/Courses/NLP/Project/data/dev_facts_0_7405.csv', header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5850c793b2f94dfda553abf9fa2ea9e4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=7405.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtCI0p0uUQ-_",
        "outputId": "9bac95ed-281d-4066-cbc3-0cc6bfbc2a5d"
      },
      "source": [
        "len(data_df[\"answer\"][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PENZB-8l20pn"
      },
      "source": [
        "#evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXEbhh8KgdF5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aEt01xRqfuf"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "contains_combined_fact=0\n",
        "for index, row in data_df.iterrows():\n",
        "  answers= get_candidate_passages(row.question)\n",
        "  temp_df= pd.DataFrame(answers, columns=['answer'])\n",
        "  temp_df['question']=row.question\n",
        "  dataloader= create_dataloader(tokenizer, temp_df)\n",
        "  pos_prob= []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      ip_ids, masks= batch\n",
        "      score = model(ip_ids, attention_mask=masks)\n",
        "      pos_prob.append(score.cpu().detach().numpy())\n",
        "  \n",
        "  temp_df['score']=pos_prob\n",
        "  temp_df= temp_df.sort_values(by='score', ascending=False)\n",
        "  top_k_ans= temp_df[0:20].answer.tolist()\n",
        "  org_ans = row.answer\n",
        "  r=[]\n",
        "  for ans in top_k_ans:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "  print(index)\n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, top_k_ans, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, top_k_ans, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, top_k_ans, 20))\n",
        "print(\"Precision @5: \",mean(precision_at_5))\n",
        "print(\"Precision @10: \",mean(precision_at_10))\n",
        "print(\"Recall @5: \",mean(recall_at_5))\n",
        "print(\"Recall @10: \",mean(recall_at_10))\n",
        "print(\"Recall @20: \",mean(recall_at_20))\n",
        "print(\"Mean Average Precision: \",mean_average_precision(rs))\n",
        "print(\"Mean Reciprocal Rank: \",mean_reciprocal_rank(rs))\n",
        "print(\"contains_combined_fact\",contains_combined_fact)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt1b3wyM78bB"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "contains_fact2=0\n",
        "for index, row in data_df.iterrows():\n",
        "  answers= get_candidate_passages(row.question)\n",
        "  temp_df= pd.DataFrame(answers, columns=['answer'])\n",
        "  temp_df['question']=row.question\n",
        "  dataloader= create_dataloader(tokenizer, temp_df)\n",
        "  pos_prob= []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      ip_ids, masks= batch\n",
        "      score = model(ip_ids, attention_mask=masks)\n",
        "      pos_prob.append(score.cpu().detach().numpy())\n",
        "  \n",
        "  temp_df['score']=pos_prob\n",
        "  temp_df= temp_df.sort_values(by='score', ascending=False)\n",
        "  top_k_ans= temp_df[0:20].answer.tolist()\n",
        "\n",
        "  org_ans=[]\n",
        "  org_ans.append(row.fact_2)\n",
        "\n",
        "  r=[]\n",
        "  for ans in top_k_ans:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "  for ans in top_k_ans:\n",
        "    if ans==row.fact_2:\n",
        "      contains_fact2+=1\n",
        "  # if(index==1000):\n",
        "  #   break\n",
        "  print(index)\n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, top_k_ans, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, top_k_ans, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, top_k_ans, 20))\n",
        "print(\"QASC DEV experiment 4 with considering only fact2\")\n",
        "print(\"Precision @5: \",mean(precision_at_5))\n",
        "print(\"Precision @10: \",mean(precision_at_10))\n",
        "print(\"Recall @5: \",mean(recall_at_5))\n",
        "print(\"Recall @10: \",mean(recall_at_10))\n",
        "print(\"Recall @20: \",mean(recall_at_20))\n",
        "print(\"Mean Average Precision: \",mean_average_precision(rs))\n",
        "print(\"Mean Reciprocal Rank: \",mean_reciprocal_rank(rs))\n",
        "print(\"contains_fact2\",contains_fact2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxInJfWEyAax"
      },
      "source": [
        "## **Classification Baseline Model experimentation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCmV-03JyuPF"
      },
      "source": [
        "# Loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "59beab1e60574211b78658e61f912dce",
            "5862c39e769449229c2730252155646c",
            "c2d4a6ffbf144a19848fa6280492c603",
            "3e342b9bf194419384556fb01706b46d",
            "a831ce2136f74039828ad7b0a49d77eb",
            "43e4b5c9efcd4a0199722908c2d31f26",
            "ed78796a66f745ab9b3f703c5f1ff2b2",
            "e4ff6f4bdb72483abb430497242c7526",
            "230297ed4ac342eba963c9f517c8bc87",
            "503a46a458644170bbaf7b6f48103d74",
            "b9c18a54531d4570b0455fdab8fb6ec1",
            "f7c42494efb34251a830acfe1d044cce",
            "075e7ca44a7c4a968975ffaf4adbf845",
            "67a885628b594e14b3dc2836a36b5769",
            "5de3df6f542c4842988ce173586608cf",
            "0aed7a693fbc45b2acca38d3db91da19"
          ]
        },
        "id": "aMuw6dbZyybu",
        "outputId": "db13b868-c52f-4153-db15-cbfae8323e0f"
      },
      "source": [
        "#classification model\n",
        "\n",
        "model_path= '/content/drive/MyDrive/NLP/Project/models/sia_trained_roberta_model'\n",
        "model= RobertaForSequenceClassification.from_pretrained(model_path)\n",
        "lr_weights= torch.load(join(model_path, 'model_state.bin'))\n",
        "#model.out.load_state_dict(lr_weights)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /content/drive/My Drive/QASC-DATASET/data/QASC_Dataset/sia_experiment/roberta_cls_model/ were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/My Drive/QASC-DATASET/data/QASC_Dataset/sia_experiment/roberta_cls_model/ and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59beab1e60574211b78658e61f912dce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "230297ed4ac342eba963c9f517c8bc87",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSODcdhOydHj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzsRWz3s83qk"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "contains_combined_fact=0\n",
        "for index, row in data_df.iterrows():\n",
        "  print(index)\n",
        "  answers= get_candidate_passages(row.question)\n",
        "  temp_df= pd.DataFrame(answers, columns=['answer'])\n",
        "  temp_df['question']=row.question\n",
        "  dataloader= create_dataloader(tokenizer, temp_df)\n",
        "  pos_prob= []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      ip_ids, masks= batch\n",
        "      logits = model(ip_ids, attention_mask=masks)\n",
        "      logits= logits[0].squeeze(0)\n",
        "      pred_logits = logits.cpu().detach().numpy()\n",
        "      pos_prob.append(pred_logits[1])\n",
        "  \n",
        "  temp_df['score']=pos_prob\n",
        "  temp_df= temp_df.sort_values(by='score', ascending=False)\n",
        "  top_k_ans= temp_df[0:20].answer.tolist()\n",
        "\n",
        "  # doc= doc_passages[str(row.DocumentID)]\n",
        "  # passages_no= row.RelevantPassages.split(',')\n",
        "  org_ans=[]\n",
        "  # for no in passages_no:\n",
        "  #   org_ans.append(doc[str(no)])\n",
        "  org_ans.append(row.combined_fact)\n",
        "  r=[]\n",
        "  for ans in top_k_ans:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "  for ans in top_k_ans:\n",
        "    if ans==row.combined_fact:\n",
        "      contains_combined_fact+=1\n",
        "  \n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, top_k_ans, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, top_k_ans, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, top_k_ans, 20))\n",
        "\n",
        "print(mean(precision_at_5))\n",
        "print(mean(precision_at_10))\n",
        "print(mean(recall_at_5))\n",
        "print(mean(recall_at_10))\n",
        "print(mean(recall_at_20))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))\n",
        "print(\"combined fact:\",contains_combined_fact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ1cfKp-WKd2",
        "outputId": "a3d56b77-55da-48a1-e544-8044a0c3ba52"
      },
      "source": [
        "print(mean(precision_at_5))\n",
        "print(mean(precision_at_10))\n",
        "print(mean(recall_at_5))\n",
        "print(mean(recall_at_10))\n",
        "print(mean(recall_at_20))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))\n",
        "print(\"combined fact:\",contains_combined_fact)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0023758099352051837\n",
            "0.003455723542116631\n",
            "0.011879049676025918\n",
            "0.032397408207343416\n",
            "0.12419006479481641\n",
            "0.014276984723627631\n",
            "0.013826532359672\n",
            "combined fact: 127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csaY4js98CrV"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "contains_all_three=0\n",
        "contains12=0\n",
        "contains1c=0\n",
        "contains2c=0\n",
        "for index, row in data_df.iterrows():\n",
        "  print(index)\n",
        "  answers= get_candidate_passages(row.question)\n",
        "  temp_df= pd.DataFrame(answers, columns=['answer'])\n",
        "  temp_df['question']=row.question\n",
        "  dataloader= create_dataloader(tokenizer, temp_df)\n",
        "  pos_prob= []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      ip_ids, masks= batch\n",
        "      logits = model(ip_ids, attention_mask=masks)\n",
        "      logits= logits[0].squeeze(0)\n",
        "      pred_logits = logits.cpu().detach().numpy()\n",
        "      pos_prob.append(pred_logits[1])\n",
        "  \n",
        "  temp_df['score']=pos_prob\n",
        "  temp_df= temp_df.sort_values(by='score', ascending=False)\n",
        "  top_k_ans= temp_df[0:20].answer.tolist()\n",
        "\n",
        "  org_ans=[]\n",
        "  org_ans.append(row.combined_fact)\n",
        "  org_ans.append(row.fact_1)\n",
        "  org_ans.append(row.fact_2)\n",
        "  r=[]\n",
        "  for ans in top_k_ans:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "  if ((row.fact_1 in top_k_ans)and (row.fact_2 in top_k_ans) and (row.combined_fact in top_k_ans)):\n",
        "    contains_all_three+=1\n",
        "  if ((row.fact_1 in top_k_ans)and (row.fact_2 in top_k_ans) and (row.combined_fact not in top_k_ans)):\n",
        "    contains12+=1\n",
        "  if ((row.fact_1 in top_k_ans) and (row.combined_fact in top_k_ans) and (row.fact_2 not in top_k_ans)):\n",
        "    contains1c+=1\n",
        "  if ((row.fact_2 in top_k_ans) and (row.combined_fact in top_k_ans)and (row.fact_1 not in top_k_ans)):\n",
        "    contains2c+=1\n",
        "\n",
        "  \n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, top_k_ans, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, top_k_ans, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, top_k_ans, 20))\n",
        "  \n",
        "\n",
        "print(mean(precision_at_5))\n",
        "print(mean(precision_at_10))\n",
        "print(mean(recall_at_5))\n",
        "print(mean(recall_at_10))\n",
        "print(mean(recall_at_20))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))\n",
        "print(\"contains_all_three\",contains_all_three)\n",
        "print(\"contains_fact1_and_fact2\",contains12)\n",
        "print(\"contains_fact1_and_factc\",contains1c)\n",
        "print(\"contains_fact2_and_factc\",contains2c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qgk97LT7_CVY"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "contains_fact1=0\n",
        "\n",
        "for index, row in data_df.iterrows():\n",
        "  print(index)\n",
        "  answers= get_candidate_passages(row.question)\n",
        "  temp_df= pd.DataFrame(answers, columns=['answer'])\n",
        "  temp_df['question']=row.question\n",
        "  dataloader= create_dataloader(tokenizer, temp_df)\n",
        "  pos_prob= []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      ip_ids, masks= batch\n",
        "      logits = model(ip_ids, attention_mask=masks)\n",
        "      logits= logits[0].squeeze(0)\n",
        "      pred_logits = logits.cpu().detach().numpy()\n",
        "      pos_prob.append(pred_logits[1])\n",
        "  \n",
        "  temp_df['score']=pos_prob\n",
        "  temp_df= temp_df.sort_values(by='score', ascending=False)\n",
        "  top_k_ans= temp_df[0:20].answer.tolist()\n",
        "\n",
        "  # doc= doc_passages[str(row.DocumentID)]\n",
        "  # passages_no= row.RelevantPassages.split(',')\n",
        "  org_ans=[]\n",
        "  # for no in passages_no:\n",
        "  #   org_ans.append(doc[str(no)])\n",
        "  # org_ans.append(row.combined_fact)\n",
        "  org_ans.append(row.fact_1)\n",
        "  # org_ans.append(row.fact_2)\n",
        "  r=[]\n",
        "  for ans in top_k_ans:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "  for ans in top_k_ans:\n",
        "    if ans==row.fact_1:\n",
        "      contains_fact1+=1\n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, top_k_ans, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, top_k_ans, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, top_k_ans, 20))\n",
        "\n",
        "print(mean(precision_at_5))\n",
        "print(mean(precision_at_10))\n",
        "print(mean(recall_at_5))\n",
        "print(mean(recall_at_10))\n",
        "print(mean(recall_at_20))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))\n",
        "print(\"contains_fact1\",contains_fact1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX7KH-7q_LUB"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "contains_fact2=0\n",
        "\n",
        "for index, row in data_df.iterrows():\n",
        "  print(index)\n",
        "  answers= get_candidate_passages(row.question)\n",
        "  temp_df= pd.DataFrame(answers, columns=['answer'])\n",
        "  temp_df['question']=row.question\n",
        "  dataloader= create_dataloader(tokenizer, temp_df)\n",
        "  pos_prob= []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      ip_ids, masks= batch\n",
        "      logits = model(ip_ids, attention_mask=masks)\n",
        "      logits= logits[0].squeeze(0)\n",
        "      pred_logits = logits.cpu().detach().numpy()\n",
        "      pos_prob.append(pred_logits[1])\n",
        "  \n",
        "  temp_df['score']=pos_prob\n",
        "  temp_df= temp_df.sort_values(by='score', ascending=False)\n",
        "  top_k_ans= temp_df[0:20].answer.tolist()\n",
        "\n",
        "  # doc= doc_passages[str(row.DocumentID)]\n",
        "  # passages_no= row.RelevantPassages.split(',')\n",
        "  org_ans=[]\n",
        "  # for no in passages_no:\n",
        "  #   org_ans.append(doc[str(no)])\n",
        "  # org_ans.append(row.combined_fact)\n",
        "  # org_ans.append(row.fact_1)\n",
        "  org_ans.append(row.fact_2)\n",
        "  r=[]\n",
        "  for ans in top_k_ans:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "  for ans in top_k_ans:\n",
        "    if ans==row.fact_2:\n",
        "      contains_fact2+=1\n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, top_k_ans, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, top_k_ans, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, top_k_ans, 20))\n",
        "\n",
        "print(mean(precision_at_5))\n",
        "print(mean(precision_at_10))\n",
        "print(mean(recall_at_5))\n",
        "print(mean(recall_at_10))\n",
        "print(mean(recall_at_20))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))\n",
        "print(\"contains_fact2_and_factc\",contains_fact2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9t_-BOEN2wi"
      },
      "source": [
        "# **BM25 Baseline Model Result Evaluation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WPeyetUPgki"
      },
      "source": [
        "def get_candidate_passages(query):\n",
        "\n",
        "  candidate_passages=[]\n",
        "  scores=[]\n",
        "  hits = searcher.search(query, k=50)\n",
        "  # return the first top 10 hits:\n",
        "  for hit in hits:\n",
        "    doc = searcher.doc(str(hit.docid))\n",
        "    candidate_passages.append(doc.raw().replace('\"', ''))\n",
        "    scores.append(hit.score)\n",
        "  \n",
        "  return candidate_passages,scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADpgu9oTOv2T"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_BXtyKbAcfI"
      },
      "source": [
        "precision_at_5=[]\n",
        "precision_at_10=[]\n",
        "recall_at_5=[]\n",
        "recall_at_10=[]\n",
        "recall_at_20=[]\n",
        "rs=[]\n",
        "contains_combined_fact=0\n",
        "for index, row in data_df.iterrows():\n",
        "  # if index  >   2000:\n",
        "  #   break;\n",
        "  print(index)\n",
        "  answers,score= get_candidate_passages(row.question)\n",
        "  temp_df= pd.DataFrame(list(zip(answers,score)), columns=['answer','score'])\n",
        "  temp_df['question']=row.question\n",
        "  # dataloader= create_dataloader(tokenizer, temp_df)\n",
        "  # pos_prob= []\n",
        "  temp_df= temp_df.sort_values(by='score', ascending=False)\n",
        "  top_k_ans= temp_df[0:20].answer.tolist()\n",
        "\n",
        "  # doc= doc_passages[str(row.DocumentID)]\n",
        "  # passages_no= row.RelevantPassages.split(',')\n",
        "  # org_ans=[]\n",
        "  # for no in passages_no:\n",
        "  #   org_ans.append(doc[str(no)])\n",
        "  # org_ans.append(row.combined_fact)\n",
        "  org_ans = row.answer\n",
        "  r=[]\n",
        "  for ans in top_k_ans:\n",
        "    if ans in org_ans:\n",
        "      r.append(1)\n",
        "    else:\n",
        "      r.append(0)\n",
        "  # for ans in top_k_ans:\n",
        "  #   if ans==row.combined_fact:\n",
        "  #     contains_combined_fact+=1\n",
        "  \n",
        "  rs.append(r)\n",
        "  precision_at_5.append(precision_at_k(r,5))\n",
        "  precision_at_10.append(precision_at_k(r,10))\n",
        "  recall_at_5.append(recall_at_k(org_ans, top_k_ans, 5))\n",
        "  recall_at_10.append(recall_at_k(org_ans, top_k_ans, 10))\n",
        "  recall_at_20.append(recall_at_k(org_ans, top_k_ans, 20))\n",
        "\n",
        "print(mean(precision_at_5))\n",
        "print(mean(precision_at_10))\n",
        "print(mean(recall_at_5))\n",
        "print(mean(recall_at_10))\n",
        "print(mean(recall_at_20))\n",
        "print(mean_average_precision(rs))\n",
        "print(mean_reciprocal_rank(rs))\n",
        "# print(\"combined fact:\",contains_combined_fact)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}